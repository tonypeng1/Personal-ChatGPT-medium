import anthropic
import google.generativeai as genai
from mistralai.client import MistralClient
from mysql.connector import connect, Error
import openai
from openai import OpenAIError
import streamlit as st


from delete_message import delete_the_messages_of_a_chat_session, \
                        delete_all_rows
from drop_file import increment_file_uploader_key, \
                        extract_text_from_different_file_types, \
                        save_to_mysql_message, \
                        set_both_load_and_search_sessions_to_False
from init_database import add_column_model_to_message_search_table, \
                        add_column_model_to_message_table, \
                        index_column_content_in_table_message, \
                        init_mysql_timezone, \
                        init_database_tables, \
                        modify_content_column_data_type_if_different
from init_session import get_and_set_current_session_id, \
                        load_previous_chat_session, \
                        set_only_current_session_state_to_true
from init_st_session_state import init_session_states
from load_session import load_current_date_from_database, \
                        get_the_earliest_date, \
                        load_previous_chat_session_ids, \
                        convert_date, \
                        get_summary_by_session_id_return_dic, \
                        remove_if_a_session_not_exist_in_date_range, \
                        get_available_date_range, \
                        get_current_session_date_in_message_table, \
                        set_new_session_to_false, \
                        set_load_session_to_False, \
                        set_search_session_to_False
from model_behavior import insert_initial_default_model_behavior, \
                        Load_the_last_saved_model_behavior, \
                        return_temp_and_top_p_values_from_model_behavior, \
                        return_behavior_index, \
                        save_model_behavior_to_mysql
from model_type import insert_initial_default_model_type, \
                        Load_the_last_saved_model_type, \
                        return_type_index, \
                        save_model_type_to_mysql
from save_to_html import convert_messages_to_markdown, \
                        markdown_to_html, \
                        get_summary_and_return_as_file_name, \
                        is_valid_file_name
from search_message import delete_all_rows_in_message_serach, \
                        search_full_text_and_save_to_message_search_table
from session_summary import get_session_summary_and_save_to_session_table


def start_session_save_to_mysql_and_increment_session_id(conn):
    """
    Start a new session by inserting a null value at the end timestamp column of the session table. Increment session id by 1.
    The start_timestamp column of the session will be automatically updated with the current time. See the function init_database_tables()).

    Args:
        conn: A MySQL connection object to interact with the database.

    Returns:
        None. Inserts a new session record and increments the session counter in `st.session_state.session`.
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("INSERT INTO session (end_timestamp) VALUE (null);")
            conn.commit()

            if st.session_state.session is not None:
                st.session_state.session += 1
            else:
                st.session_state.session = 1

    except Error as error:
        st.error(f"Failed to start a new session: {error}")
        raise


def end_session_save_to_mysql_and_save_summary(conn) -> None:
    """
    End the current session by updating the end_timestamp in the session table in the MySQL database.
    Get and save session summary. If the session state "session" is None, this function does nothing.

    Args:
        conn: A MySQL connection object to interact with the database.

    Returns:
        None. Updates the database with the end timestamp for the current session.
    """
    try:
        with conn.cursor() as cursor:
            sql = "UPDATE session SET end_timestamp = CURRENT_TIMESTAMP() WHERE session_id = %s;"
            val = (st.session_state.session,)
            cursor.execute(sql, val)
            conn.commit()

    except Error as error:
        st.error(f"Failed to end a session: {error}")
        raise

    get_session_summary_and_save_to_session_table(conn, chatgpt_client, st.session_state.session)  # Save session summary after ending session


def chatgpt(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Processes a chat prompt using OpenAI's ChatCompletion and updates the chat session.

    Args:
        conn: A connection object to the MySQL database.
        prompt (str): The user's input prompt to the chatbot.
        temp (float): The temperature parameter for OpenAI's ChatCompletion.
        p (float): The top_p parameter for OpenAI's ChatCompletion.
        max_tok (int): The maximum number of tokens for OpenAI's ChatCompletion.

    Raises:
        Raises an exception if there is a failure in database operations or OpenAI's API call.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            for response in chatgpt_client.chat.completions.create(
                model="gpt-4-turbo-2024-04-09",
                messages=
                    [{"role": "system", "content": model_role}] +
                    [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                    ],
                temperature=temp,
                top_p=p,
                max_tokens=max_tok,
                stream=True,
                ):
                full_response += response.choices[0].delta.content or ""
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except OpenAIError as e:
            error_response = f"An error occurred with OpenAI in getting chat response: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)
        except Exception as e:
            error_response = f"An unexpected error occurred in OpenAI API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def gemini(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Generates a response using the Gemini API.

    Args:
        conn: A MySQL connection object.
        prompt1: The user's input.
        temp: The temperature parameter for the Gemini API.
        p: The top-p parameter for the Gemini API.
        max_tok: The maximum number of tokens for the Gemini API.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            for response in  gemini_model.generate_content(
                [{"role": "user", 
                  "parts": [{
                      "text": model_role + "If you understand your role, please response 'I understand.'"
                      }]
                      },
                {"role": "model", 
                 "parts": [{"text": "I understand."}]
                 }
                ] +
                [
                {"role": m["role"] if m["role"] == "user" else "model", "parts": [{"text": m["content"]}]}
                for m in st.session_state.messages
                ],
                generation_config = genai.types.GenerationConfig(
                                    candidate_count = 1,
                                    temperature=temp,
                                    top_p=p,
                                    max_output_tokens=max_tok
                                    ),
                stream=True
                ):
                if hasattr(response, 'parts'):
                    for part in response.parts:
                        text_content = part.text
                        full_response += text_content
                else:
                    text_content += response.text
                    full_response += text_content
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except Exception as e:
            error_response = f"An unexpected error occurred in gemini API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def mistral(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Generates a response using the mistral API.

    Args:
        conn: A MySQL connection object.
        prompt1: The user's input.
        temp: The temperature parameter for the mistral API.
        p: The top-p parameter for the mistral API. 
            (ignore, error occurs if used simultaneously with temperature)
        max_tok: The maximum number of tokens for the mistral API.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""

        messages = [{
        "role": "user", "content": model_role
        }]
        for m in st.session_state.messages:
            messages.append({"role": m["role"], "content": m["content"]})

        try:
            for response in mistral_client.chat_stream(
                model=mistral_model,
                messages=messages,
                temperature=temp,
                # top_p=p,
                max_tokens=max_tok
                ):
                if response.choices[0].delta.content is not None:
                    full_response += response.choices[0].delta.content
                    message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except Exception as e:
            error_response = f"An unexpected error occurred in Mistral API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def claude(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Processes a chat prompt using Anthropic's Claude 3.5 model and updates the chat session.

    Args:
        conn: A connection object to the MySQL database.
        prompt (str): The user's input prompt to the chatbot.
        temp (float): The temperature parameter for OpenAI's ChatCompletion.
        p (float): The top_p parameter for OpenAI's ChatCompletion.
        max_tok (int): The maximum number of tokens for OpenAI's ChatCompletion.

    Raises:
        Raises an exception if there is a failure in database operations or OpenAI's API call.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            with claude_client.messages.stream(
                model=claude_model,
                system=model_role,
                messages=
                    [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                    ],
                temperature=temp,
                top_p=p,
                max_tokens=max_tok,
                ) as stream:
                for response in stream.text_stream:
                    full_response += response
                    message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except Exception as e:
            error_response = f"An unexpected error occurred in Claude 3.5 API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def together(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Processes a chat prompt using Together's ChatCompletion and updates the chat session.

    Args:
        conn: A connection object to the MySQL database.
        prompt (str): The user's input prompt to the chatbot.
        temp (float): The temperature parameter for Together's ChatCompletion.
        p (float): The top_p parameter for Together's ChatCompletion.
        max_tok (int): The maximum number of tokens for Together's ChatCompletion.

    Raises:
        Raises an exception if there is a failure in database operations or Together's API call.
    """
    role = "You are an expert programmer who helps to write and debug code based on \
            the user's request with concise explanations. \
            When rendering code samples always include the import statements if applicable. \
            When giving required code solutions include complete code with no omission. \
            When rephrasing paragraphs, use lightly casual, straight-to-the-point language."

    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            for response in together_client.chat.completions.create(
                model="codellama/CodeLlama-70b-Instruct-hf",
                messages=
                    [{"role": "system", "content": role}] +
                    [
                    {"role": m["role"], 
                     "content": f'[INST]{m["content"]}[/INST]' if m["role"] == "user" \
                     else m["content"]}
                    for m in st.session_state.messages
                    ],
                temperature=temp,
                top_p=p,
                max_tokens=max_tok,
                stream=True,
                ):
                full_response += response.choices[0].delta.content or ""
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except OpenAIError as e:
            error_response = f"An error occurred with TogetherAI in getting chat response: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)
        except Exception as e:
            error_response = f"An unexpected error occurred in TogetherAI API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def together_python(prompt1: str, temp: float, p: float, max_tok: int) -> str:
    """
    Processes a prompt using Together's Completion and updates the chat session. This
    function is not a chat, only one prompt at a time.

    Args:
        conn: A connection object to the MySQL database.
        prompt (str): The user's input prompt to the chatbot.
        temp (float): The temperature parameter for Together's ChatCompletion.
        p (float): The top_p parameter for Together's Completion.
        max_tok (int): The maximum number of tokens for Together's Completion.

    Raises:
        Raises an exception if there is a failure in database operations or Together's API call.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            for response in together_client.completions.create(
                model="codellama/CodeLlama-70b-Python-hf",
                prompt=prompt1,
                temperature=temp,
                top_p=p,
                max_tokens=max_tok,
                stream=True,
                ):
                full_response += response.choices[0].text or ""
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except OpenAIError as e:
            error_response = f"An error occurred with TogetherAI in getting chat response: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)
        except Exception as e:
            error_response = f"An unexpected error occurred in TogetherAI API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def perplexity(prompt1: str, model_role: str, temp: float, p: float, max_tok: int) -> str:
    """
    Processes a chat prompt using Perplexity OpenAI's ChatCompletion and updates the chat session.

    Args:
        conn: A connection object to the MySQL database.
        prompt (str): The user's input prompt to the chatbot.
        temp (float): The temperature parameter for OpenAI's ChatCompletion.
        p (float): The top_p parameter for OpenAI's ChatCompletion.
        max_tok (int): The maximum number of tokens for OpenAI's ChatCompletion.

    Raises:
        Raises an exception if there is a failure in database operations or OpenAI's API call.
    """
    with st.chat_message("user"):
        st.markdown(prompt1)

    with st.chat_message("assistant"):
        text = f":blue-background[:blue[**{model_name}**]]"  
        st.markdown(text)

        message_placeholder = st.empty()
        full_response = ""
        try:
            for response in perplexity_client.chat.completions.create(
                model="llama-3.1-sonar-large-128k-online",
                messages=
                    [{"role": "system", "content": model_role}] +
                    [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                    ],
                temperature=temp,
                top_p=p,
                max_tokens=max_tok,
                stream=True,
                ):
                full_response += response.choices[0].delta.content or ""
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        except OpenAIError as e:
            error_response = f"An error occurred with OpenAI in getting chat response: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)
        except Exception as e:
            error_response = f"An unexpected error occurred in OpenAI API call: {e}"
            full_response = error_response
            message_placeholder.markdown(full_response)

    return full_response


def save_session_state_messages(conn) -> None:
    """
    Iterates over messages in the session state and saves each to the message table.

    Args:
    conn: The database connection object.
    """
    for message in st.session_state.messages:
        save_to_mysql_message(conn, st.session_state.session, message["role"], 
                              message["model"], message["content"])


def determine_if_terminate_current_session_and_start_a_new_one(conn) -> None:
    """
    Determines if the current session should be terminated based on `st.session_state` and starts a new one if necessary.

    Args:
    conn: The database connection object.
    """
    state_actions = {
        'new_table': lambda: start_session_save_to_mysql_and_increment_session_id(conn),
        'new_session': lambda: (end_session_save_to_mysql_and_save_summary(conn),
                                start_session_save_to_mysql_and_increment_session_id(conn)),
        'load_history_level_2': lambda: (end_session_save_to_mysql_and_save_summary(conn),
                                         start_session_save_to_mysql_and_increment_session_id(conn),
                                         save_session_state_messages(conn),  # with messages of the already loaded old session 
                                         delete_the_messages_of_a_chat_session(conn, load_history_level_2)),  # with the old session id
        'session_different_date': lambda: (end_session_save_to_mysql_and_save_summary(conn), 
                                           delete_the_messages_of_a_chat_session(conn, st.session_state.session), 
                                           start_session_save_to_mysql_and_increment_session_id(conn), 
                                           save_session_state_messages(conn)
                                         )
    }

    for state, action in state_actions.items():
        if st.session_state.get(state):
            action()  # Executes the corresponding action for the state
            st.session_state[state] = False  # Resets the state to False
            break  # Breaks after handling a state, assuming only one state can be true at a time


def process_prompt(conn, prompt1, model_name, model_role, temperature, top_p, max_token):
    """
    This function processes a given prompt by performing the following steps:
    1. Determines if the current session should be terminated and a new one started.
    2. Appends the prompt to the session state messages with the role 'user'.
    3. Calls the appropriate model (chatgpt, gemini, or mistral) based on the model_name and passes 
        the prompt, temperature, top_p, and max_token parameters.
    4. Appends the model's response to the session state messages with the role 'assistant'.
    5. Saves the prompt and response to the MySQL database.

    Parameters:
    conn (object): The connection object to the database.
    prompt1 (str): The prompt to be processed.
    model_name (str): The name of the model to be used for processing the prompt.
    temperature (float): The temperature parameter for the model.
    top_p (float): The top_p parameter for the model.
    max_token (int): The maximum number of tokens for the model's response.

    Returns:
    None
    """
    determine_if_terminate_current_session_and_start_a_new_one(conn)
    st.session_state.messages.append({"role": "user", "model": "", "content": prompt1})
    try:
        if model_name == "gpt-4-turbo-2024-04-09":
            responses = chatgpt(prompt1, model_role, temperature, top_p, int(max_token))
        elif model_name == "claude-3-5-sonnet-20240620":
            responses = claude(prompt1, model_role, temperature, top_p, int(max_token))
        elif model_name == "gemini-1.5-pro-exp-0801":
            responses = gemini(prompt1, model_role, temperature, top_p, int(max_token))
        elif model_name == "mistral-large-latest":
            responses = mistral(prompt1, model_role, temperature, top_p, int(max_token))   
        elif model_name == "perplexity-llama-3.1-sonar-large-128k-online":
            responses = perplexity(prompt1, model_role, temperature, top_p, int(max_token))  
        else:
            raise ValueError('Model is not in the list.')

        st.session_state.messages.append({"role": "assistant", "model": model_name, 
                                      "content": responses})

        save_to_mysql_message(conn, st.session_state.session, "user", "", prompt1)
        save_to_mysql_message(conn, st.session_state.session, "assistant", model_name, responses)

    except ValueError as error:
        st.error(f"Not recognized model name: {error}")
        raise


# Get app keys
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
MISTRAL_API_KEY = st.secrets["MISTRAL_API_KEY"]
CLAUDE_API_KEY = st.secrets["ANTHROPIC_API_KEY"]
TOGETHER_API_KEY = st.secrets["TOGETHER_API_KEY"]
PERPLEXITY_API_KEY = st.secrets["PERPLEXITY_API_KEY"]

# Set gemini api configuration
genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel('gemini-1.5-pro-exp-0801')

# Set mastral api configuration
mistral_model = "mistral-large-latest"
mistral_client = MistralClient(api_key=MISTRAL_API_KEY)

# Set Claude api configuration
claude_model = "claude-3-5-sonnet-20240620"
claude_client = anthropic.Anthropic(api_key=CLAUDE_API_KEY,)

# Set chatgpt api configuration
chatgpt_client = openai.OpenAI(api_key=OPENAI_API_KEY)

# Set together api configuration
together_client = openai.OpenAI(
  api_key=TOGETHER_API_KEY,
  base_url='https://api.together.xyz/v1'
)

# Set perplexity api configuration
perplexity_client = openai.OpenAI(
    api_key=PERPLEXITY_API_KEY, 
    base_url="https://api.perplexity.ai"
    )

# Database initial operation
connection = connect(**st.secrets["mysql"])  # get database credentials from .streamlit/secrets.toml
init_database_tables(connection) # Create tables if not existing
init_mysql_timezone(connection)  # Set database global time zone to America/Chicago
modify_content_column_data_type_if_different(connection)
add_column_model_to_message_table(connection)  # Add model column to message table if not exist
add_column_model_to_message_search_table(connection) # Add model column to message_search table if not exist
index_column_content_in_table_message(connection)  # index column content in table message if not yet indexed

init_session_states()  # Initialize all streamlit session states if there is no value

## Get the current date (US central time) and the earliest date from database
today = load_current_date_from_database(connection)  
date_earlist = get_the_earliest_date(connection)

new_chat_button = st.sidebar.button(r"$\textsf{\normalsize New chat session}$", 
                                    type="primary", 
                                    key="new",
                                    on_click=increment_file_uploader_key)

# If the user clicks "New chat session" widget:
if new_chat_button:
    st.session_state.messages = []
    set_only_current_session_state_to_true("new_session")
    st.session_state.load_session = False
    st.session_state.search_session = False
    st.session_state.drop_file = False


# The following code handles the search and retreival of the messages of a chat session
# (list of all matched sessions together)
search_session = st.sidebar.button\
                (r"$\textsf{\normalsize SEARCH for a session}$", 
                 on_click=set_load_session_to_False, 
                 type="primary", 
                 key="search")

if search_session:
    st.session_state.search_session = True
    st.session_state.drop_file = False

if st.session_state.search_session:
    keywords = st.sidebar.text_input('Full-text boolean search (Add + or - for a word that must be present or absent)')
    if keywords != "":
        delete_all_rows_in_message_serach(connection)
        search_full_text_and_save_to_message_search_table(connection, keywords)

        all_dates_sessions = load_previous_chat_session_ids(connection, 'message_search', *convert_date('All dates', date_earlist, today))
        all_dates_dic = get_summary_by_session_id_return_dic(connection, all_dates_sessions)

        level_two_options_new = {
            None : {0: "None"},
            "All dates": all_dates_dic}

        # Show options as summary. The returned value is the session id of the picked session.
        load_history_level_2 = st.sidebar.selectbox(
                label="Select a chat session:",
                placeholder='Pick a session',
                options=list((level_two_options_new["All dates"]).keys()),
                index=None,
                format_func=lambda x:level_two_options_new["All dates"][x],
                on_change=set_new_session_to_false
                )

        if load_history_level_2:
            load_previous_chat_session(connection, load_history_level_2)

            if load_history_level_2 != st.session_state.session:
                set_only_current_session_state_to_true("load_history_level_2")
            else:
                st.session_state.load_history_level_2 = False  # case for current active session

            # The following code is for saving the messages to a html file.
            session_md = convert_messages_to_markdown(st.session_state.messages)
            session_html = markdown_to_html(session_md)

            file_name = get_summary_and_return_as_file_name(connection, load_history_level_2) + ".html"

            download_chat_session = st.sidebar.download_button(
                label="Save it to a .html file",
                data=session_html,
                file_name=file_name,
                mime="text/markdown",
            )
            if download_chat_session:
                if is_valid_file_name(file_name):
                    st.success("Data saved.")
                else:
                    st.error(f"The file name '{file_name}' is not a valid file name. File not saved!", icon="🚨")

            delete_a_session = st.sidebar.button("Delete it from database")
            st.sidebar.markdown("""----------""")

            if delete_a_session:
                st.session_state.delete_session = True


st.title("Personal LLM APP")
st.sidebar.title("Options")

# Handle model type. The type chosen will be reused rather than using a default value. 
# If the type table is empty, set the initial type to "Deterministic".
insert_initial_default_model_type(connection, 'gemini-1.5-pro-exp-0801')

Load_the_last_saved_model_type(connection)  # load from database and save to session_state
type_index = return_type_index(st.session_state.type)  # from string to int (0 to 4)

model_name = st.sidebar.radio(
                                label="Choose model:",
                                options=(
                                    "gpt-4-turbo-2024-04-09",
                                    "claude-3-5-sonnet-20240620", 
                                    "mistral-large-latest",
                                    "perplexity-llama-3.1-sonar-large-128k-online",
                                    # "CodeLlama-70b-Instruct-hf",
                                    "gemini-1.5-pro-exp-0801"
                                 ),
                                index=type_index,
                                key="type1"
                            )

if model_name != st.session_state.type:  # only save to database if type is newly clicked 
    save_model_type_to_mysql(connection, model_name)  


# Handle model behavior. The behavior chosen will be reused rather than using a default value. 
# If the behavior table is empty, set the initial behavior to "Deterministic".
insert_initial_default_model_behavior(connection, 'Deterministic (T=0.0, top_p=0.2)')

Load_the_last_saved_model_behavior(connection)  # load from database and save to session_state
(temperature, top_p) = return_temp_and_top_p_values_from_model_behavior(st.session_state.behavior)
behavior_index = return_behavior_index(st.session_state.behavior)  # from string to int (0 to 4)

behavior = st.sidebar.selectbox(
    label="Select the behavior of your model",
    placeholder='Pick a behavior',
    options=['Deterministic (T=0.0, top_p=0.2)', 'Conservative (T=0.3, top_p=0.4)', 
             'Balanced (T=0.6, top_p=0.6)', 'Diverse (T=0.8, top_p=0.8)', 'Creative (T=1.0, top_p=1.0)'],
    index=behavior_index,
    key="behavior1"
    )

if behavior != st.session_state.behavior:  # only save to database if behavior is newly clicked 
    save_model_behavior_to_mysql(connection, behavior)  

max_token = st.sidebar.number_input(
    label="Select the max number of tokens the model can generate",
    min_value=500,
    max_value=4000,
    value=1000,
    step=500
    )

# Initiate a session (either display the current active session in the database or 
# start a new session)
if "session" not in st.session_state:
    get_and_set_current_session_id(connection)

    if st.session_state.session is not None:
        load_previous_chat_session(connection, st.session_state.session)

        # The code below is used to handle a current active session across different dates and a new prompt is added.
        current_session_datetime = \
        get_current_session_date_in_message_table(connection, st.session_state.session)
        if current_session_datetime is not None:
            current_session_date = current_session_datetime[0].date()

            if today != current_session_date:  # If a new session ignore the line below.
                set_only_current_session_state_to_true("session_different_date")

    else:
        set_only_current_session_state_to_true("new_table")  # The case where the session table is empty


# The following code handles the retreival of the messages of a previous chat session
# (list of session_ids of different date ranges)
load_session = st.sidebar.button \
            (r"$\textsf{\normalsize LOAD a session}$", 
            on_click=set_search_session_to_False, 
            type="primary", 
            key="load")

if load_session:
    st.session_state.load_session = True
    st.session_state.drop_file = False

if st.session_state.load_session:
    today_sessions = load_previous_chat_session_ids(connection, 'message', *convert_date('Today', date_earlist, today))
    yesterday_sessions = load_previous_chat_session_ids(connection, 'message', *convert_date('Yesterday', date_earlist, today))
    seven_days_sessions = load_previous_chat_session_ids(connection, 'message', *convert_date('Previous 7 days', date_earlist, today))
    thirty_days_sessions = load_previous_chat_session_ids(connection, 'message', *convert_date('Previous 30 days', date_earlist, today))
    older_sessions = load_previous_chat_session_ids(connection, 'message', *convert_date('Older', date_earlist, today))

    today_dic = get_summary_by_session_id_return_dic(connection, today_sessions)
    yesterday_dic = get_summary_by_session_id_return_dic(connection, yesterday_sessions)
    seven_days_dic = get_summary_by_session_id_return_dic(connection, seven_days_sessions)
    thirty_days_dic = get_summary_by_session_id_return_dic(connection, thirty_days_sessions)
    older_dic = get_summary_by_session_id_return_dic(connection, older_sessions)

    level_two_options = {
        None : {0: "None"},
        "Today" : today_dic,
        "Yesterday" : yesterday_dic,
        "Previous 7 days" : seven_days_dic,
        "Previous 30 days" : thirty_days_dic,
        "Older" : older_dic
    }

    level_two_options_new = remove_if_a_session_not_exist_in_date_range(level_two_options)
    level_one_options = get_available_date_range(level_two_options_new)

    # Only shows the data ranges with saved chat sessions.
    load_history_level_1 = st.sidebar.selectbox(
        label='Select a chat date:',
        placeholder='Pick a date',
        options=level_one_options,
        index=None,
        key="first_level"
        )

    # Show options as summary. The returned value is the session id of the picked session.
    load_history_level_2 = st.sidebar.selectbox(
            label="Select a chat session:",
            placeholder='Pick a session',
            options=list((level_two_options_new[load_history_level_1]).keys()),
            index=None,
            format_func=lambda x:level_two_options_new[load_history_level_1][x],
            on_change=set_new_session_to_false
            )

    if load_history_level_2:
        load_previous_chat_session(connection, load_history_level_2)

        if load_history_level_2 != st.session_state.session:
            set_only_current_session_state_to_true("load_history_level_2")
        else:
            st.session_state.load_history_level_2 = False  # case for current active session

        # The following code is for saving the messages to a html file.
        session_md = convert_messages_to_markdown(st.session_state.messages)
        session_html = markdown_to_html(session_md)

        file_name = get_summary_and_return_as_file_name(connection, load_history_level_2) + ".html"

        download_chat_session = st.sidebar.download_button(
            label="Save it to a .html file",
            data=session_html,
            file_name=file_name,
            mime="text/markdown",
        )
        if download_chat_session:
            if is_valid_file_name(file_name):
                st.success("Data saved.")
            else:
                st.error(f"The file name '{file_name}' is not a valid file name. File not saved!", icon="🚨")

        delete_a_session = st.sidebar.button("Delete it from database")
        st.sidebar.markdown("""----------""")

        if delete_a_session:
            st.session_state.delete_session = True



# The following code handles dropping a file from the local computer
drop_file = st.sidebar.button \
            (r"$\textsf{\normalsize Drop a file to LLM}$", 
            type="primary", 
            key="drop")

if drop_file:
    st.session_state.drop_file = True
    st.session_state.load_session = False
    st.session_state.search_session = False

if st.session_state.drop_file:
    dropped_files = st.sidebar.file_uploader("Drop a file or multiple files (.txt, .rtf, .pdf, .csv)", 
                                            accept_multiple_files=True,
                                            on_change=set_both_load_and_search_sessions_to_False,
                                            key=st.session_state.file_uploader_key)

    if dropped_files == []:  # when a file is removed, reset the question to False
        st.session_state.question = False

    question = ""
    prompt_f = ""
    if dropped_files != [] \
        and not st.session_state.question:
            question = st.sidebar.text_area(
                "Any question about the files? (to be inserted at start of the files)", 
                placeholder="None")

            for dropped_file in dropped_files:   
                file_prompt = extract_text_from_different_file_types(dropped_file)
                prompt_f += file_prompt

            prompt_f = question + " " + prompt_f

            to_chatgpt = st.sidebar.button("Send to LLM API")
            st.sidebar.markdown("""----------""")

            if dropped_files != [] and to_chatgpt:
                # and (to_chatgpt and question != ""):
                st.session_state.question = True
                st.session_state.send_drop_file = True
                st.session_state.drop_file = False


# The following code handles the deletion of all chat history. The code needs to be
# after messages printing in order to show confirmation at end of messages.
st.sidebar.markdown("""----------""")
empty_database = st.sidebar.button(
    r"$\textsf{\normalsize Delete the entire chat history}$", type="primary")

if empty_database:
    st.session_state.messages = []
    st.session_state.empty_data = True
    st.error(r"$\textsf{\large Do you really wanna DELETE THE ENTIRE CHAT HISTORY?}$", \
             icon="🔥")

    placeholder_confirmation_all = st.empty()

    if st.session_state.empty_data:
        with placeholder_confirmation_all.container():
            confirmation_2 = st.selectbox(
                label="CONFIRM YOUR ANSWER (If you choose 'Yes', ALL CHAT HISTORY in the local \
                    database will be deleted):",
                placeholder="Pick a choice",
                options=['No', 'Yes'],
                index=None,
                key="second_confirmation"
            )
        if confirmation_2 == 'Yes':
            delete_all_rows(connection)
            st.warning("All data in the database deleted.", icon="🚨")
            st.session_state.empty_data = False
            st.session_state.new_session = True
            st.session_state.session = None
            st.rerun()
        elif confirmation_2 == 'No':
            st.success("Data not deleted.")
            st.session_state.empty_data = False


# Print each message on page (this code prints pre-existing message before calling chatgpt(), 
# where the latest messages will be printed.) if not loading or searching a previous session.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] == "user":
            st.markdown(message["content"])
        else:
            if message["model"] == "":
                st.markdown(message["content"])
            else:
                text = message["model"]
                text = f":blue-background[:blue[**{text}**]]"   
                st.markdown(text)
                st.markdown(message["content"])


# The following code handles previous session deletion after uploading. The code needs to be
# after messages printing in order to show confirmation at end of messages.
if st.session_state.delete_session:
    if load_history_level_2 != 0:
        st.session_state.delete_session = True
        st.error("Do you really wanna delete this chat history?", icon="🚨")
    else:
        st.warning("No saved session loaded. Please select one from the above drop-down lists.")
        st.session_state.delete_session = False

    placeholder_confirmation_sesson = st.empty()

    if st.session_state.delete_session:
        with placeholder_confirmation_sesson.container():
            confirmation_1 = st.selectbox(
                label="Confirm your answer (If you choose 'Yes', this chat history of thie loaded session will be deleted):",
                placeholder="Pick a choice",
                options=['No', 'Yes'],
                index=None
            )
        if confirmation_1 == 'Yes':
            delete_the_messages_of_a_chat_session(connection, load_history_level_2)
            st.session_state.delete_session = False
            st.session_state.messages = []
            st.session_state.new_session = True
            st.rerun()
        elif confirmation_1 == 'No':
            st.success("Data not deleted.")
            st.session_state.delete_session = False



# The following code handles model API call and new chat session creation (if necessary) before sending
# the API call. 
model_role = "You are an experienced software engineer based in Austin, Texas, \
            predominantly working with Kafka, java, flink, Kafka-connect, ververica-platform. \
            You also work on machine learning projects using python, interested in generative AI and LLMs. \
            When rendering code samples always include the import statements. \
            When giving required code solutions include complete code with no omission. \
            When giving long responses add the source of the information as URLs. \
            You are fine with strong opinion as long as the source of the information can be pointed out \
            and always question my understanding. \
            When rephrasing paragraphs, use lightly casual, straight-to-the-point language."

if prompt := st.chat_input("What is up?"):
    process_prompt(connection, prompt, model_name, model_role, temperature, top_p, max_token)

if st.session_state.send_drop_file:
    process_prompt(connection, prompt_f, model_name, model_role, temperature, top_p, max_token)

    st.session_state.send_drop_file = False
    increment_file_uploader_key()  # so that a new file_uploader shows up whithour the files
    st.rerun()

connection.close()